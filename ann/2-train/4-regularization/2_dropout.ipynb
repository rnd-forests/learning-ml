{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout** is one of the most popular regularization techniques for DNN.\n",
    "\n",
    "The idea behind dropout is simple. At every training step, every neuron (including input neurons but excluding output neurons) has a probability $p$ of being *temporarily dropped out* which means these neurons will be ignored during a training step. However, they may become active again in the subsequent training steps.\n",
    "\n",
    "Hyperparameter $p$ is called *dropout rate* and typically takes the value of 0.5 or 50%. After training process, neurons won't be dropped anymore.\n",
    "\n",
    "At each training step, a unique neural network is generated. Since each neuron can be either present or absent, there'll be $2^N$ possible networks ($N$ is the number of droppable neurons).\n",
    "\n",
    "As a result, it is nearly impossible to generate a same neural network twice. All the generated neural networks are not independent since they share many weights; however, they're all different.\n",
    "\n",
    "Imagine with dropout, we have an ensemble of all smaller neural networks (like the Random Forest algorithm), and the result will be the average of those networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "Suppose that $p = 0.5$, during testing a neuron will be connected to twice as many input neurons as it was (on average) during training. As a result, we need to multiply each neuron's input connection weights by 0.5 after training. Otherwise, each neuron will get a input signal twice as large as what the network was trained on (worse performance).\n",
    "\n",
    "**Generally, we need to multiply each input connection weight by the KEEP PROBABILITY $(1 - p)$ after training. Alternatively, we can divide each neuron's output by the keep probability during training. These two methods are not the same; however, they both work well.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorFlow, we can use `tf.layers.dropout()` function to implement Dropout for DNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is overfitting, we should increase the dropout rate. Conversely, we should decrease the dropout rate if the model underfits the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that applying dropout would slow down the convergence, but it **usually** results in a much better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 784\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropout_rate = 0.25 # 1 - keep_prob\n",
    "\n",
    "# This placeholder is used to turn on/off training mode for other variables\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "# Apply dropout for input neurons\n",
    "X_dropped = tf.layers.dropout(X, rate=dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    # Use dropped X for the first hidden layer\n",
    "    hidden1 = tf.layers.dense(X_dropped, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    \n",
    "    # Use dropout for the second hidden layer\n",
    "    hidden1_dropped = tf.layers.dropout(hidden1, rate=dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_dropped, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    \n",
    "    # Use dropout for the output layer\n",
    "    hidden2_dropped = tf.layers.dropout(hidden2, rate=dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_dropped, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0 -- Test Accuracy: 0.9596\n",
      "Epoch: 1 -- Test Accuracy: 0.9666\n",
      "Epoch: 2 -- Test Accuracy: 0.9684\n",
      "Epoch: 3 -- Test Accuracy: 0.9728\n",
      "Epoch: 4 -- Test Accuracy: 0.9764\n",
      "Epoch: 5 -- Test Accuracy: 0.9777\n",
      "Epoch: 6 -- Test Accuracy: 0.9789\n",
      "Epoch: 7 -- Test Accuracy: 0.9812\n",
      "Epoch: 8 -- Test Accuracy: 0.9808\n",
      "Epoch: 9 -- Test Accuracy: 0.9811\n",
      "Epoch: 10 -- Test Accuracy: 0.9819\n",
      "Epoch: 11 -- Test Accuracy: 0.9831\n",
      "Epoch: 12 -- Test Accuracy: 0.9827\n",
      "Epoch: 13 -- Test Accuracy: 0.9839\n",
      "Epoch: 14 -- Test Accuracy: 0.9836\n",
      "Epoch: 15 -- Test Accuracy: 0.985\n",
      "Epoch: 16 -- Test Accuracy: 0.9843\n",
      "Epoch: 17 -- Test Accuracy: 0.9836\n",
      "Epoch: 18 -- Test Accuracy: 0.9843\n",
      "Epoch: 19 -- Test Accuracy: 0.9838\n",
      "Epoch: 20 -- Test Accuracy: 0.9844\n",
      "Epoch: 21 -- Test Accuracy: 0.9845\n",
      "Epoch: 22 -- Test Accuracy: 0.9845\n",
      "Epoch: 23 -- Test Accuracy: 0.9844\n",
      "Epoch: 24 -- Test Accuracy: 0.9844\n",
      "Epoch: 25 -- Test Accuracy: 0.985\n",
      "Epoch: 26 -- Test Accuracy: 0.9843\n",
      "Epoch: 27 -- Test Accuracy: 0.9848\n",
      "Epoch: 28 -- Test Accuracy: 0.9851\n",
      "Epoch: 29 -- Test Accuracy: 0.9851\n",
      "Epoch: 30 -- Test Accuracy: 0.9852\n",
      "Epoch: 31 -- Test Accuracy: 0.9849\n",
      "Epoch: 32 -- Test Accuracy: 0.985\n",
      "Epoch: 33 -- Test Accuracy: 0.9847\n",
      "Epoch: 34 -- Test Accuracy: 0.9852\n",
      "Epoch: 35 -- Test Accuracy: 0.9848\n",
      "Epoch: 36 -- Test Accuracy: 0.9849\n",
      "Epoch: 37 -- Test Accuracy: 0.9848\n",
      "Epoch: 38 -- Test Accuracy: 0.9846\n",
      "Epoch: 39 -- Test Accuracy: 0.9847\n",
      "Epoch: 40 -- Test Accuracy: 0.9847\n",
      "Epoch: 41 -- Test Accuracy: 0.9851\n",
      "Epoch: 42 -- Test Accuracy: 0.9849\n",
      "Epoch: 43 -- Test Accuracy: 0.9848\n",
      "Epoch: 44 -- Test Accuracy: 0.9848\n",
      "Epoch: 45 -- Test Accuracy: 0.9848\n",
      "Epoch: 46 -- Test Accuracy: 0.9847\n",
      "Epoch: 47 -- Test Accuracy: 0.9847\n",
      "Epoch: 48 -- Test Accuracy: 0.9849\n",
      "Epoch: 49 -- Test Accuracy: 0.9848\n",
      "Epoch: 50 -- Test Accuracy: 0.9849\n",
      "Epoch: 51 -- Test Accuracy: 0.9849\n",
      "Epoch: 52 -- Test Accuracy: 0.9849\n",
      "Epoch: 53 -- Test Accuracy: 0.9849\n",
      "Epoch: 54 -- Test Accuracy: 0.9848\n",
      "Epoch: 55 -- Test Accuracy: 0.9849\n",
      "Epoch: 56 -- Test Accuracy: 0.9849\n",
      "Epoch: 57 -- Test Accuracy: 0.9849\n",
      "Epoch: 58 -- Test Accuracy: 0.9849\n",
      "Epoch: 59 -- Test Accuracy: 0.9849\n",
      "Epoch: 60 -- Test Accuracy: 0.9849\n",
      "Epoch: 61 -- Test Accuracy: 0.9849\n",
      "Epoch: 62 -- Test Accuracy: 0.9849\n",
      "Epoch: 63 -- Test Accuracy: 0.9849\n",
      "Epoch: 64 -- Test Accuracy: 0.9849\n",
      "Epoch: 65 -- Test Accuracy: 0.9849\n",
      "Epoch: 66 -- Test Accuracy: 0.9849\n",
      "Epoch: 67 -- Test Accuracy: 0.9849\n",
      "Epoch: 68 -- Test Accuracy: 0.9849\n",
      "Epoch: 69 -- Test Accuracy: 0.9849\n",
      "Epoch: 70 -- Test Accuracy: 0.9849\n",
      "Epoch: 71 -- Test Accuracy: 0.9849\n",
      "Epoch: 72 -- Test Accuracy: 0.9849\n",
      "Epoch: 73 -- Test Accuracy: 0.9849\n",
      "Epoch: 74 -- Test Accuracy: 0.9849\n",
      "Epoch: 75 -- Test Accuracy: 0.9849\n",
      "Epoch: 76 -- Test Accuracy: 0.9849\n",
      "Epoch: 77 -- Test Accuracy: 0.9849\n",
      "Epoch: 78 -- Test Accuracy: 0.9849\n",
      "Epoch: 79 -- Test Accuracy: 0.9849\n",
      "Epoch: 80 -- Test Accuracy: 0.9849\n",
      "Epoch: 81 -- Test Accuracy: 0.9849\n",
      "Epoch: 82 -- Test Accuracy: 0.9849\n",
      "Epoch: 83 -- Test Accuracy: 0.9849\n",
      "Epoch: 84 -- Test Accuracy: 0.9849\n",
      "Epoch: 85 -- Test Accuracy: 0.9849\n",
      "Epoch: 86 -- Test Accuracy: 0.9849\n",
      "Epoch: 87 -- Test Accuracy: 0.9849\n",
      "Epoch: 88 -- Test Accuracy: 0.9849\n",
      "Epoch: 89 -- Test Accuracy: 0.9849\n",
      "Epoch: 90 -- Test Accuracy: 0.9849\n",
      "Epoch: 91 -- Test Accuracy: 0.9849\n",
      "Epoch: 92 -- Test Accuracy: 0.9849\n",
      "Epoch: 93 -- Test Accuracy: 0.9849\n",
      "Epoch: 94 -- Test Accuracy: 0.9849\n",
      "Epoch: 95 -- Test Accuracy: 0.9849\n",
      "Epoch: 96 -- Test Accuracy: 0.9849\n",
      "Epoch: 97 -- Test Accuracy: 0.9849\n",
      "Epoch: 98 -- Test Accuracy: 0.9849\n",
      "Epoch: 99 -- Test Accuracy: 0.9849\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        \n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(\"Epoch:\", epoch, \"--\", \"Test Accuracy:\", acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
