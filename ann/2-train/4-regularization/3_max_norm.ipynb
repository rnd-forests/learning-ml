{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind Max-Norm is that it constrains the weights $w$ of the incoming connections such that ${\\lVert w \\lVert}_2 \\le r$ where $r$ is the max-norm hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement this regularizer by computing $\\lVert w \\lVert_2$ after each training step and *clipping* $w$ if needed $w \t\\leftarrow w{\\dfrac{r}{\\lVert w \\lVert_2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing $r$ increases the amount of regularization which, in turn, reduces overfitting.\n",
    "\n",
    "If we're not using Batch Normalization, Max-Norm cal also helps alleviate vanishing/exploding gradients problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "n_inputs = 784\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1/kernel:0\n",
      "hidden1/bias:0\n",
      "hidden2/kernel:0\n",
      "hidden2/bias:0\n",
      "outputs/kernel:0\n",
      "outputs/bias:0\n",
      "train/global_step:0\n",
      "hidden1/kernel/Momentum:0\n",
      "hidden1/bias/Momentum:0\n",
      "hidden2/kernel/Momentum:0\n",
      "hidden2/bias/Momentum:0\n",
      "outputs/kernel/Momentum:0\n",
      "outputs/bias/Momentum:0\n",
      "hidden3/dense/kernel:0\n",
      "hidden3/dense/bias:0\n",
      "hidden2/kernel:0\n",
      "hidden2/bias:0\n",
      "hidden2/kernel/Momentum:0\n",
      "hidden2/bias/Momentum:0\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"hidden3\", reuse=tf.AUTO_REUSE):\n",
    "    hidden3 = tf.layers.dense(hidden2, 100, activation=tf.nn.relu)\n",
    "    \n",
    "for variable in tf.global_variables():\n",
    "    print(variable.name)\n",
    "\n",
    "hidden2_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden2\")\n",
    "for variable in hidden2_variables:\n",
    "    print(variable.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "init = tf.global_variables_initializer()\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Max-norm threshold\n",
    "threshold = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"hidden1/kernel:0\", shape=(784, 300), dtype=float32_ref)\n",
      "Tensor(\"hidden2/kernel:0\", shape=(300, 100), dtype=float32_ref)\n"
     ]
    }
   ],
   "source": [
    "# Accessing weights of each layer\n",
    "w1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "w2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "\n",
    "print(w1)\n",
    "print(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"clip_by_norm:0\", shape=(784, 300), dtype=float32)\n",
      "Tensor(\"Assign:0\", shape=(784, 300), dtype=float32_ref)\n",
      "Tensor(\"clip_by_norm_1:0\", shape=(300, 100), dtype=float32)\n",
      "Tensor(\"Assign_1:0\", shape=(300, 100), dtype=float32_ref)\n"
     ]
    }
   ],
   "source": [
    "# Messy approach to implement max-norm\n",
    "# We're going to create a clip_weights node that will clip the weights\n",
    "clipped_w1 = tf.clip_by_norm(w1, clip_norm=threshold, axes=1)\n",
    "w1 = tf.assign(w1, clipped_w1)\n",
    "print(clipped_w1)\n",
    "print(w1)\n",
    "\n",
    "clipped_w2 = tf.clip_by_norm(w2, clip_norm=threshold, axes=1)\n",
    "w2 = tf.assign(w2, clipped_w2)\n",
    "print(clipped_w2)\n",
    "print(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 -- Test Accuracy: 0.9637\n",
      "Epoch: 1 -- Test Accuracy: 0.9647\n",
      "Epoch: 2 -- Test Accuracy: 0.9715\n",
      "Epoch: 3 -- Test Accuracy: 0.9792\n",
      "Epoch: 4 -- Test Accuracy: 0.9787\n",
      "Epoch: 5 -- Test Accuracy: 0.9815\n",
      "Epoch: 6 -- Test Accuracy: 0.9824\n",
      "Epoch: 7 -- Test Accuracy: 0.9835\n",
      "Epoch: 8 -- Test Accuracy: 0.9831\n",
      "Epoch: 9 -- Test Accuracy: 0.9836\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            # Evaluate clip weights nodes\n",
    "            w1.eval()\n",
    "            w2.eval()\n",
    "        \n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(\"Epoch:\", epoch, \"--\", \"Test Accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Better approach\n",
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\", collection=\"max_norm\"):\n",
    "    def max_norm(w):\n",
    "        clipped_w = tf.clip_by_norm(w, clip_norm=threshold, axes=axes)\n",
    "        w = tf.assign(w, clipped_w)\n",
    "        \n",
    "        # Add clipped weights to a collection for fetching later\n",
    "        tf.add_to_collection(collection, w)\n",
    "        \n",
    "        # Max-norm regularization doesn't require adding regularization loss term to the cost function\n",
    "        # so we return None here\n",
    "        return None \n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm_regularizer = max_norm_regularizer(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, \n",
    "                              kernel_regularizer=max_norm_regularizer, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_regularizer, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 -- Test Accuracy: 0.9523\n",
      "Epoch: 1 -- Test Accuracy: 0.9599\n",
      "Epoch: 2 -- Test Accuracy: 0.9697\n",
      "Epoch: 3 -- Test Accuracy: 0.9747\n",
      "Epoch: 4 -- Test Accuracy: 0.9753\n",
      "Epoch: 5 -- Test Accuracy: 0.9785\n",
      "Epoch: 6 -- Test Accuracy: 0.9773\n",
      "Epoch: 7 -- Test Accuracy: 0.9783\n",
      "Epoch: 8 -- Test Accuracy: 0.9784\n",
      "Epoch: 9 -- Test Accuracy: 0.9807\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "clip_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_weights)\n",
    "        \n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(\"Epoch:\", epoch, \"--\", \"Test Accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default DNN configuration:\n",
    "- Initialization: He Initialization\n",
    "- Activation function: ELU\n",
    "- Normalization: Batch normalization\n",
    "- Regularization: Dropout\n",
    "- Optimizer: Adam\n",
    "- Learning rate schedule: None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
