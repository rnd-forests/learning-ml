{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad (Adaptive Gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equations (Vectorization form):\n",
    "\n",
    "![](https://photos-5.dropbox.com/t/2/AABph95VK3YO2qoY7ZyNwoaWtirMX8XX6kCqxxxQTIkmoQ/12/63047491/png/32x32/1/_/1/2/Screenshot%20from%202017-10-18%2010-41-12.png/EITw7jAYtz4gBygH/ccbTm0wuHxin8D72tfhzTig_ZqMZcYd4SzQD3tBcfus?size=2048x1536&size_mode=3)\n",
    "\n",
    "Non-vectorization form:\n",
    "\n",
    "![](https://photos-3.dropbox.com/t/2/AACw0eC5SS9-EacX4XIVaxb9et0IabIaeg_zinPE52hgGA/12/63047491/png/32x32/1/_/1/2/Screenshot%20from%202017-10-18%2010-56-12.png/EITw7jAYuT4gBygH/NmwLTZ2ohr6RmKdMwStQQiaOyZaUmmO93VFpbb3EsMk?size=2048x1536&size_mode=3)\n",
    "\n",
    "![](https://photos-1.dropbox.com/t/2/AADlAkyE6NuIH8rRT0leYfrM75L-s6y_LotctG1AKmIuuw/12/63047491/png/32x32/1/_/1/2/Screenshot%20from%202017-10-18%2010-56-39.png/EITw7jAYuT4gBygH/bgr0-OWax20p_Ylnu9-obfBtCLF5aYiDdCVk0NfZv-g?size=2048x1536&size_mode=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **AdaGrad** algorithm works by scaling down the gradient vector along the steepest dimensions.\n",
    "\n",
    "- The algorithm maintains a vector s which consists of square of gradients (obtained by using element-wist multiplication of gradients). In other words, each element s<sub>i</sub> accumulates the squares of the partial derivative of the cost function with regard to parameter θ<sub>i</sub>. If the cost function is steep along the i<sup>th</sup> dimension (bigger derivative), then s<sub>i</sub> will get larger after each iteration.\n",
    "\n",
    "- The second step of the algorithm is almost identical to vanilla Gradient Descent with one big difference. The gradient vector is scaled down by a factor of `sqrt(s + ε)` (obtained by using element-wise division). Epsilon is called the smoothing term which is used to prevent division by zero (usually take the value of 10<sup>-10</sup>). The equivalent non-vectorized form is shown in above figure.\n",
    "\n",
    "The general idea of this algorithm is that it decays the learning rate faster for steep dimension and lower for dimension with gentler slopes. This process is called **Adaptive Learning Rate**. One benefit of this approach is that it requires much less tuning of the learning rate hyperparameter.\n",
    "\n",
    "*Note that this algorithm is not good when training neural networks as it often stops too early. Specifically, the learning rate gets scaled down so much that the algorithm ends up stopping before reaching the global minimum. Therefore, we should not use this algorithm to train neural networks (it may be suitable for simple tasks like Linear Regression)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "tf.reset_default_graph()\n",
    "n_inputs = 784\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.elu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xen = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xen, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.78 Test accuracy: 0.8544\n",
      "1 Train accuracy: 0.86 Test accuracy: 0.8842\n",
      "2 Train accuracy: 0.96 Test accuracy: 0.8984\n",
      "3 Train accuracy: 0.91 Test accuracy: 0.9005\n",
      "4 Train accuracy: 0.89 Test accuracy: 0.9082\n",
      "5 Train accuracy: 0.93 Test accuracy: 0.9125\n",
      "6 Train accuracy: 0.91 Test accuracy: 0.9151\n",
      "7 Train accuracy: 0.92 Test accuracy: 0.9168\n",
      "8 Train accuracy: 0.96 Test accuracy: 0.9183\n",
      "9 Train accuracy: 0.93 Test accuracy: 0.919\n",
      "10 Train accuracy: 0.91 Test accuracy: 0.9233\n",
      "11 Train accuracy: 0.92 Test accuracy: 0.9235\n",
      "12 Train accuracy: 0.94 Test accuracy: 0.9228\n",
      "13 Train accuracy: 0.93 Test accuracy: 0.9236\n",
      "14 Train accuracy: 0.92 Test accuracy: 0.93\n",
      "15 Train accuracy: 0.99 Test accuracy: 0.9269\n",
      "16 Train accuracy: 0.93 Test accuracy: 0.9277\n",
      "17 Train accuracy: 0.95 Test accuracy: 0.9297\n",
      "18 Train accuracy: 0.95 Test accuracy: 0.9309\n",
      "19 Train accuracy: 0.91 Test accuracy: 0.931\n",
      "20 Train accuracy: 0.96 Test accuracy: 0.9315\n",
      "21 Train accuracy: 0.94 Test accuracy: 0.9328\n",
      "22 Train accuracy: 0.97 Test accuracy: 0.9338\n",
      "23 Train accuracy: 0.96 Test accuracy: 0.9352\n",
      "24 Train accuracy: 0.92 Test accuracy: 0.9333\n",
      "25 Train accuracy: 0.95 Test accuracy: 0.937\n",
      "26 Train accuracy: 0.96 Test accuracy: 0.9363\n",
      "27 Train accuracy: 0.94 Test accuracy: 0.9382\n",
      "28 Train accuracy: 0.96 Test accuracy: 0.9398\n",
      "29 Train accuracy: 0.95 Test accuracy: 0.9391\n",
      "30 Train accuracy: 0.95 Test accuracy: 0.9409\n",
      "31 Train accuracy: 0.96 Test accuracy: 0.9407\n",
      "32 Train accuracy: 0.95 Test accuracy: 0.9408\n",
      "33 Train accuracy: 0.91 Test accuracy: 0.9429\n",
      "34 Train accuracy: 0.98 Test accuracy: 0.9409\n",
      "35 Train accuracy: 0.94 Test accuracy: 0.9427\n",
      "36 Train accuracy: 0.92 Test accuracy: 0.9444\n",
      "37 Train accuracy: 0.94 Test accuracy: 0.9438\n",
      "38 Train accuracy: 0.93 Test accuracy: 0.9462\n",
      "39 Train accuracy: 0.96 Test accuracy: 0.9447\n",
      "40 Train accuracy: 0.9 Test accuracy: 0.9464\n",
      "41 Train accuracy: 0.92 Test accuracy: 0.9479\n",
      "42 Train accuracy: 0.96 Test accuracy: 0.9479\n",
      "43 Train accuracy: 0.99 Test accuracy: 0.9486\n",
      "44 Train accuracy: 0.96 Test accuracy: 0.95\n",
      "45 Train accuracy: 0.92 Test accuracy: 0.9503\n",
      "46 Train accuracy: 0.95 Test accuracy: 0.9497\n",
      "47 Train accuracy: 0.96 Test accuracy: 0.949\n",
      "48 Train accuracy: 0.98 Test accuracy: 0.9504\n",
      "49 Train accuracy: 0.96 Test accuracy: 0.9518\n",
      "50 Train accuracy: 0.94 Test accuracy: 0.9518\n",
      "51 Train accuracy: 0.97 Test accuracy: 0.9523\n",
      "52 Train accuracy: 0.99 Test accuracy: 0.9533\n",
      "53 Train accuracy: 0.94 Test accuracy: 0.9531\n",
      "54 Train accuracy: 0.97 Test accuracy: 0.953\n",
      "55 Train accuracy: 0.97 Test accuracy: 0.9537\n",
      "56 Train accuracy: 0.93 Test accuracy: 0.9541\n",
      "57 Train accuracy: 0.94 Test accuracy: 0.9535\n",
      "58 Train accuracy: 0.92 Test accuracy: 0.9538\n",
      "59 Train accuracy: 0.94 Test accuracy: 0.9539\n",
      "60 Train accuracy: 0.96 Test accuracy: 0.9551\n",
      "61 Train accuracy: 0.97 Test accuracy: 0.9564\n",
      "62 Train accuracy: 0.98 Test accuracy: 0.9553\n",
      "63 Train accuracy: 0.99 Test accuracy: 0.9568\n",
      "64 Train accuracy: 0.95 Test accuracy: 0.9562\n",
      "65 Train accuracy: 0.95 Test accuracy: 0.956\n",
      "66 Train accuracy: 0.96 Test accuracy: 0.9583\n",
      "67 Train accuracy: 0.99 Test accuracy: 0.958\n",
      "68 Train accuracy: 0.94 Test accuracy: 0.9578\n",
      "69 Train accuracy: 0.9 Test accuracy: 0.9573\n",
      "70 Train accuracy: 0.96 Test accuracy: 0.9584\n",
      "71 Train accuracy: 0.98 Test accuracy: 0.9578\n",
      "72 Train accuracy: 0.98 Test accuracy: 0.9582\n",
      "73 Train accuracy: 0.97 Test accuracy: 0.9583\n",
      "74 Train accuracy: 0.98 Test accuracy: 0.9576\n",
      "75 Train accuracy: 0.97 Test accuracy: 0.9594\n",
      "76 Train accuracy: 0.98 Test accuracy: 0.9601\n",
      "77 Train accuracy: 0.96 Test accuracy: 0.961\n",
      "78 Train accuracy: 0.96 Test accuracy: 0.9594\n",
      "79 Train accuracy: 0.96 Test accuracy: 0.9603\n",
      "80 Train accuracy: 0.99 Test accuracy: 0.9602\n",
      "81 Train accuracy: 0.98 Test accuracy: 0.9607\n",
      "82 Train accuracy: 0.98 Test accuracy: 0.9615\n",
      "83 Train accuracy: 0.97 Test accuracy: 0.9625\n",
      "84 Train accuracy: 0.93 Test accuracy: 0.9607\n",
      "85 Train accuracy: 0.95 Test accuracy: 0.9609\n",
      "86 Train accuracy: 0.97 Test accuracy: 0.9632\n",
      "87 Train accuracy: 0.95 Test accuracy: 0.9616\n",
      "88 Train accuracy: 0.95 Test accuracy: 0.9634\n",
      "89 Train accuracy: 0.99 Test accuracy: 0.9628\n",
      "90 Train accuracy: 0.95 Test accuracy: 0.9625\n",
      "91 Train accuracy: 0.98 Test accuracy: 0.9634\n",
      "92 Train accuracy: 0.95 Test accuracy: 0.9631\n",
      "93 Train accuracy: 0.97 Test accuracy: 0.9628\n",
      "94 Train accuracy: 0.96 Test accuracy: 0.9637\n",
      "95 Train accuracy: 0.97 Test accuracy: 0.9629\n",
      "96 Train accuracy: 0.97 Test accuracy: 0.964\n",
      "97 Train accuracy: 0.99 Test accuracy: 0.9642\n",
      "98 Train accuracy: 0.97 Test accuracy: 0.9639\n",
      "99 Train accuracy: 0.98 Test accuracy: 0.9646\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(mnist.test.labels) // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
